import datetime
import hashlib
import gpytorch
import pandas as pd
from sklearn.model_selection import train_test_split

import torch

# Check if CUDA is available and set the default tensor type to cuda.FloatTensor
if torch.cuda.is_available():
    torch.set_default_tensor_type(torch.cuda.FloatTensor)
    print("Using CUDA:", torch.cuda.get_device_name(0))


class GPRegressionModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


# Define a training routine
def train_model(df):

    # Assuming df is your DataFrame and you have already processed your features
    X = df[["AL", "SD", "age", "gender"]].values
    y = df["reflection_S"].values
    df["gender"] = df["gender"].astype("category").cat.codes

    # Splitting the dataset
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Converting arrays to PyTorch tensors
    X_train = torch.tensor(X_train, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.float32)
    X_test = torch.tensor(X_test, dtype=torch.float32)
    y_test = torch.tensor(y_test, dtype=torch.float32)

    # If you're using a GPU
    if torch.cuda.is_available():
        X_train, y_train = X_train.cuda(), y_train.cuda()
        X_test, y_test = X_test.cuda(), y_test.cuda()

    # Define a GPyTorch model and likelihood here
    # model = MyGPModel(...)
    # likelihood = gpytorch.likelihoods.GaussianLikelihood()

    # Train your model on X_train, y_train
    # Evaluate your model on X_test, y_test

    likelihood = gpytorch.likelihoods.GaussianLikelihood()
    model = GPRegressionModel(X_train, y_train, likelihood=likelihood)

    if torch.cuda.is_available():
        model = model.cuda()
        likelihood = likelihood.cuda()

    optimizer = torch.optim.Adam(
        [
            {"params": model.parameters()},  # Includes GaussianLikelihood parameters
        ],
        lr=0.1,
    )

    # model = GPModel(X, y, likelihood)
    # Switch model to training mode
    model.train()
    likelihood.train()

    # Use the adam optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

    # "Loss" for GPs - the marginal log likelihood
    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

    training_iterations = 50
    for i in range(training_iterations):
        # Zero backprop gradients
        optimizer.zero_grad()
        # Get output from model
        output = model(X_train)
        # Calc loss and backprop derivatives
        loss = -mll(output, y_train)
        loss.backward()
        print("Iter %d/%d - Loss: %.3f" % (i + 1, training_iterations, loss.item()))
        optimizer.step()

    # Validation
    model.eval()
    likelihood.eval()
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        test_preds = model(X_test)
        observed_pred = likelihood(test_preds)
        # Evaluate the predictions
        mean = observed_pred.mean
        # Compare mean to the true test_y values for validation metrics
        mse = torch.mean((mean - y_test) ** 2)
        rmse = torch.sqrt(mse)
        print(f"Mean Squared Error: {mse.item()}")
        print(f"Root Mean Squared Error: {rmse.item()}")

    # Return the trained model
    return model